---
title: "Assignment _Chapter_06"
author: "XIAOYAN YUE"
output: 
  html_document: 
    keep_md: yes
---
##6E1 State the three motivating criteria that define information entropy. Try to express each in your own words.
(1)The variation of information entropy are continuous.

(2)The information entropy vary with the number of possible events.The more number of possible events, the larger of the information entropy should be.

(3)The information entropy of different possible events could be additive.

##6E2 Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
calculate formula:

H(p) = - (0.3*log0.3)+(0.7*log0.7))

R calculation:
```{r}
p <- c( 0.3 , 0.7 )
-sum( p*log(p) )
```

##6E3 Suppose a four-sided die is loaded such that, when tossed onto a table, it shows ¡°1¡± 20%, ¡°2¡± 25%, ¡±3¡± 25%, and ¡±4¡± 30% of the time. What is the entropy of this die?
```{r}
p <- c(0.20,0.25,0.25,0.30 )
-sum( p*log(p) )
```
##6E4 Suppose another four-sided die is loaded such that it never shows ¡°4¡±. The other three sides show equally often. What is the entropy of this die?
```{r}
p <- c(1/3,1/3,1/3)
-sum( p*log(p) )
```
##6M1 Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?

They are three kinds of information criterions which provide estimates of the average out-of-sample deviance to nominate the model that will produce the best predictions.

Amongst, **AIC** is the oldest and _**most restrictive**_, which is an approximation that is reliable based on three restrictive conditions: the priors are flat or overwhelmed by the likelihood, the posterior distribution is approximately multivariate Gaussian, and the sample size is much larger than the number of parameters. Comparatively, **DIC** is a more _**common**_ criterion which accommondates informative priors but still assumes that the posterior is multivariate Gaussian and the sample size is much larger than the number of parameters. While, **WAIC** is the _**most general**_ criterion, which does not require a multivariate Gaussian posterior.

When we assume that the priors are flat or overwhelmed by the likelihood, we would like to transform a more general criterion into a less general one. 

##6M5 Provide an informal explanation of why informative priors reduce overfitting.

Informative priors did not simply assume that every parameter value is equally plausible and gave us more information of the parameters, which could prevent a model from getting overexcited by the training sample due to over-represented likelihood in the posteriors. 

##6M6 Provide an information explanation of why overly informative priors result in underfitting.

If a informative prior is too skeptical, it would restrict the regular features that a model must learn from the training sample. 

##6J1: explore how the code in Code Block 6.16 works.  Explain what is happening in each line.

The aim of this code _**R code 6.16**_is to calculate the log-likelihood of each observation _i_ at each sample _s_ from the posterior: 
```{r}
n_samples <- 1000 # set a matrix from 1 to 1000 to refer to the obervations
ll <- sapply( 1:n_samples , # X, a list from 1 to 1000 observations
              function(s) { #the function to be applied to each element of X to calcualte the log-likelihood of each observation
                mu <- post$a[s] + post$b[s]*cars$speed # take out the observations at each sample for the posterior
                dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )#calculate the log-likelihood for each observation
              } )
```

